{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#               Forearm Keypoint Detection"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries and models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from keras.models import load_model\n",
    "import numpy as np\n",
    "from PIL import Image, ImageOps\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.transform import resize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n"
     ]
    }
   ],
   "source": [
    "model = load_model('Keypoints-model.h5')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## conversion of an image to 160 x 90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = 'your-image-path'\n",
    "img = Image.open(img_path)\n",
    "\n",
    "# Try to get the EXIF data\n",
    "try:\n",
    "    exif_data = img._getexif()\n",
    "    if 274 in exif_data:    # 274 is the EXIF tag for Orientation\n",
    "        orientation = exif_data[274]\n",
    "\n",
    "        # Handle the orientation\n",
    "        if orientation == 3:\n",
    "            img = img.rotate(180, expand=True)\n",
    "        elif orientation == 6:\n",
    "            img = img.rotate(-90, expand=True)\n",
    "        elif orientation == 8:\n",
    "            img = img.rotate(90, expand=True)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "image = np.array(img)\n",
    "\n",
    "# Resize image using PIL\n",
    "new_height = 1280\n",
    "aspect_ratio = image.shape[1]/image.shape[0] \n",
    "new_width = round(new_height * aspect_ratio)\n",
    "img = img.resize((new_width, new_height))\n",
    "\n",
    "# Convert PIL Image back to NumPy array\n",
    "resized_image = np.array(img)\n",
    "\n",
    "# Crop image to required size\n",
    "def crop_image(image, new_shape):\n",
    "    current_shape = image.shape\n",
    "    y_start = (current_shape[0] - new_shape[0]) // 2\n",
    "    y_end = y_start + new_shape[0]\n",
    "    x_start = (current_shape[1] - new_shape[1]) // 2\n",
    "    x_end = x_start + new_shape[1]\n",
    "    cropped_image = image[y_start:y_end, x_start:x_end, :]\n",
    "    return cropped_image\n",
    "\n",
    "cropped_image = crop_image(resized_image, (1280, 720, 3))\n",
    "\n",
    "# Final resize for CNN model\n",
    "final_image = resize(cropped_image, (160, 90, 3), mode='constant')\n",
    "\n",
    "# Show final image\n",
    "plt.imshow(final_image)\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## live video of 160 x 90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the crop function\n",
    "def crop_image(image, new_shape):\n",
    "    current_shape = image.shape\n",
    "    y_start = (current_shape[0] - new_shape[0]) // 2\n",
    "    y_end = y_start + new_shape[0]\n",
    "    x_start = (current_shape[1] - new_shape[1]) // 2\n",
    "    x_end = x_start + new_shape[1]\n",
    "    cropped_image = image[y_start:y_end, x_start:x_end, :]\n",
    "    return cropped_image\n",
    "\n",
    "# Create a VideoCapture object\n",
    "cap = cv2.VideoCapture(0)  # 0 is usually the built-in webcam\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        # Read a frame from the camera\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        # If frame is read correctly, ret is True\n",
    "        if not ret:\n",
    "            print(\"Can't receive frame. Exiting ...\")\n",
    "            break\n",
    "\n",
    "        # Convert the frame to PIL Image\n",
    "        img = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "        # Resize the frame\n",
    "        new_height = 1280\n",
    "        aspect_ratio = img.size[0]/img.size[1]  # PIL Image.size is in (width, height) format\n",
    "        new_width = round(new_height * aspect_ratio)\n",
    "        img = img.resize((new_width, new_height))\n",
    "\n",
    "        # Convert PIL Image back to NumPy array\n",
    "        resized_frame = np.array(img)\n",
    "\n",
    "        # Crop the frame to required size\n",
    "        cropped_frame = crop_image(resized_frame, (1280, 720, 3))\n",
    "\n",
    "        # Final resize for CNN model\n",
    "        final_frame = resize(cropped_frame, (160, 90, 3), mode='constant')\n",
    "\n",
    "        # Convert the final frame back to BGR color format for displaying with cv2.imshow\n",
    "        final_frame = cv2.cvtColor(final_frame.astype('float32'), cv2.COLOR_RGB2BGR)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        # Display the resulting frame\n",
    "        cv2.imshow('frame', final_frame)\n",
    "\n",
    "        # Wait for the user to press 'q' key to stop the loop\n",
    "        if cv2.waitKey(1) == ord('q'):\n",
    "            break\n",
    "finally:\n",
    "    # Release the VideoCapture object and close windows\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## live keypoints\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 112ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define the crop function\n",
    "def crop_image(image, new_shape):\n",
    "    current_shape = image.shape\n",
    "    y_start = (current_shape[0] - new_shape[0]) // 2\n",
    "    y_end = y_start + new_shape[0]\n",
    "    x_start = (current_shape[1] - new_shape[1]) // 2\n",
    "    x_end = x_start + new_shape[1]\n",
    "    cropped_image = image[y_start:y_end, x_start:x_end, :]\n",
    "    return cropped_image\n",
    "\n",
    "# Load the pre-trained model\n",
    "\n",
    "# Create a VideoCapture object\n",
    "cap = cv2.VideoCapture(0)  # 0 is usually the built-in webcam\n",
    "# codec = 0x47504A4D  # MJPG\n",
    "# camera.set(cv2.CAP_PROP_FPS, 30.0)\n",
    "# camera.set(cv2.CAP_PROP_FOURCC, codec)\n",
    "# camera.set(cv2.CAP_PROP_FRAME_WIDTH, 1920)\n",
    "# camera.set(cv2.CAP_PROP_FRAME_HEIGHT, 1080)\n",
    "\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        # Read a frame from the camera\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        # If frame is read correctly, ret is True\n",
    "        if not ret:\n",
    "            print(\"Can't receive frame. Exiting ...\")\n",
    "            break\n",
    "\n",
    "        # Convert the frame to PIL Image\n",
    "        img = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "        # Resize the frame\n",
    "        new_height = 1280\n",
    "        aspect_ratio = img.size[0] / img.size[1]  # PIL Image.size is in (width, height) format\n",
    "        new_width = round(new_height * aspect_ratio)\n",
    "        img = img.resize((new_width, new_height))\n",
    "\n",
    "        # Convert PIL Image back to NumPy array\n",
    "        resized_frame = np.array(img)\n",
    "\n",
    "        # Crop the frame to the required size\n",
    "        cropped_frame = crop_image(resized_frame, (1280, 720, 3))\n",
    "        cropped_np = np.array(cropped_frame) #convert to np array\n",
    "        resized_pre_image = resize(cropped_np, (160, 90, 3), mode='constant')\n",
    "        image_batch = np.expand_dims(resized_pre_image, axis=0)\n",
    "        # add an extra dimension for the batch\n",
    "        image_batch = np.expand_dims(resized_pre_image, axis=0)\n",
    "# ... Your previous code up to model.predict ...\n",
    "\n",
    "        predictions = model.predict(image_batch)\n",
    "\n",
    "        # Reshape the predicted keypoints array to a (4, 2) matrix\n",
    "        predicted_keypoints = predictions.reshape((4, 2))\n",
    "\n",
    "        # Convert the keypoints to integers\n",
    "        predicted_keypoints = predicted_keypoints.astype(int)\n",
    "        cropped_frame_rgb = cv2.cvtColor(cropped_frame, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "\n",
    "        # Copy the frame so you don't modify the original\n",
    "        drawn_frame = cropped_frame_rgb.copy()\n",
    "\n",
    "        # Plot the keypoints\n",
    "        for i in range(predicted_keypoints.shape[0]):\n",
    "            x = predicted_keypoints[i][0]\n",
    "            y = predicted_keypoints[i][1]\n",
    "            if x < 0:\n",
    "                x = 0\n",
    "            if y < 0:\n",
    "                y = 0\n",
    "\n",
    "            cv2.circle(drawn_frame, (x, y), radius=10, color=(0, 255, 0), thickness=-1)\n",
    "\n",
    "        # Display the resulting frame\n",
    "        cv2.imshow('frame', drawn_frame)\n",
    "\n",
    "        # Wait for the user to press 'q' key to stop the loop\n",
    "        if cv2.waitKey(1) == ord('q'):\n",
    "            break\n",
    "finally:\n",
    "    # Release the VideoCapture object and close windows\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "camera = cv2.VideoCapture(0)\n",
    "\n",
    "codec = 0x47504A4D  # MJPG\n",
    "camera.set(cv2.CAP_PROP_FPS, 30.0)\n",
    "camera.set(cv2.CAP_PROP_FOURCC, codec)\n",
    "camera.set(cv2.CAP_PROP_FRAME_WIDTH, 1920)\n",
    "camera.set(cv2.CAP_PROP_FRAME_HEIGHT, 1080)\n",
    "\n",
    "while (1):\n",
    "    retval, im = camera.read(0)\n",
    "    cv2.imshow(\"image\", im)\n",
    "\n",
    "    k = cv2.waitKey(1) & 0xff\n",
    "    if k == 27:\n",
    "        break\n",
    "\n",
    "camera.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the crop function\n",
    "def crop_image(image, new_shape):\n",
    "    current_shape = image.shape\n",
    "    y_start = (current_shape[0] - new_shape[0]) // 2\n",
    "    y_end = y_start + new_shape[0]\n",
    "    x_start = (current_shape[1] - new_shape[1]) // 2\n",
    "    x_end = x_start + new_shape[1]\n",
    "    cropped_image = image[y_start:y_end, x_start:x_end, :]\n",
    "    return cropped_image\n",
    "\n",
    "# Load the pre-trained model\n",
    "model = load_model('/Users/abdul/Documents/VS-Code/42028-Deep-Learning-and-CNN/Code/model_with_background.h5')\n",
    "\n",
    "# Create a VideoCapture object\n",
    "cap = cv2.VideoCapture(0)  # 0 is usually the built-in webcam\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        # Read a frame from the camera\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        # If frame is read correctly, ret is True\n",
    "        if not ret:\n",
    "            print(\"Can't receive frame. Exiting ...\")\n",
    "            break\n",
    "\n",
    "        # Convert the frame to PIL Image\n",
    "        img = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "        # Resize the frame\n",
    "        new_height = 1280\n",
    "        aspect_ratio = img.size[0] / img.size[1]  # PIL Image.size is in (width, height) format\n",
    "        new_width = round(new_height * aspect_ratio)\n",
    "        img = img.resize((new_width, new_height))\n",
    "\n",
    "        # Convert PIL Image back to NumPy array\n",
    "        resized_frame = np.array(img)\n",
    "\n",
    "        # Crop the frame to the required size\n",
    "        cropped_frame = crop_image(resized_frame, (1280, 720, 3))\n",
    "        cropped_np = np.array(cropped_image) #convert to np array\n",
    "        resized_pre_image = resize(cropped_np, (160, 90, 3), mode='constant')\n",
    "        image_batch = np.expand_dims(resized_pre_image, axis=0)\n",
    "        # add an extra dimension for the batch\n",
    "        image_batch = np.expand_dims(resized_pre_image, axis=0)\n",
    "\n",
    "# Make predictions\n",
    "        predictions = model.predict(image_batch)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "      \n",
    "        cropped_frame_rgb = cv2.cvtColor(cropped_frame, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "\n",
    "        # Display the resulting frame\n",
    "        cv2.imshow('frame', cropped_frame_rgb)\n",
    "\n",
    "        # Wait for the user to press 'q' key to stop the loop\n",
    "        if cv2.waitKey(1) == ord('q'):\n",
    "            break\n",
    "finally:\n",
    "    # Release the VideoCapture object and close windows\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Create a VideoCapture object\n",
    "cap = cv2.VideoCapture(0)  # 0 is usually the built-in webcam\n",
    "\n",
    "# Set camera resolution\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)  # Replace with desired width\n",
    "cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)  # Replace with desired height\n",
    "\n",
    "while True:\n",
    "    # Read frame from the camera\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    # Display the frame\n",
    "    cv2.imshow(\"Video\", frame)\n",
    "\n",
    "    # Wait for the 'q' key to exit\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the VideoCapture object and close any open windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
